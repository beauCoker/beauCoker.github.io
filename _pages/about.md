---
layout: about
title: about
permalink: /
description: 

profile:
  align: right
  image: IMG_0531 2.jpg
  address: >
    <p>this is me</p>

news: false  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

Hello there! I'm a postdoc in the [Center for Theoretical Neuroscience](https://ctn.zuckermaninstitute.columbia.edu) at Columbia University's [Zuckerman Institute](https://zuckermaninstitute.columbia.edu){:target="\_blank"}, working with [John Cunningham](https://sites.stat.columbia.edu/cunningham/) on properties of variational inference. Previously, I completed a PhD in biostatistics at Harvard, advised by [Finale Doshi-Velez](https://dtak.github.io){:target="\_blank"} and [Brent Coull](https://www.hsph.harvard.edu/brent-coull/){:target="\_blank"}, and an MS in statistics at Duke, advised by  [Cynthia Rudin](https://users.cs.duke.edu/~cynthia/){:target="\_blank"}.

I'm interested in probabilistic machine learning, particularly Bayesian neural networks (BNNs) and Gaussian processes (GPs). I think about questions like:
- How do we design priors that encode meaningful functional properties?
- What are the theoretical connections between BNNs and GPs, especially under approximate inference?
- Recently: How can we leverage implicit regularization in probabilistic modeling? 

Here's a <a href="https://virtual.aistats.org/virtual/2022/poster/3368">talk</a> I gave at AISTATS about mean-field variational BNNs.

**Contact:** bc3107 [AT] columbia [DOT] edu